"""
Test script for enhanced granular analysis and visualization features
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from feature_engineering import FeatureEngineer
import pandas as pd
import numpy as np
from datetime import datetime, timedelta


def test_granular_features():
    """Test granular minute analysis features"""
    print("=" * 60)
    print("Testing Granular Minute Analysis Features")
    print("=" * 60)
    
    # Create sample minute data with valid OHLC relationships
    dates = pd.date_range(start='2024-01-01 09:30', periods=200, freq='1min')
    
    # Generate base prices
    open_prices = np.random.uniform(100, 110, 200)
    close_prices = np.random.uniform(100, 110, 200)
    
    # Ensure high >= max(open, close) and low <= min(open, close)
    high_prices = np.maximum(open_prices, close_prices) + np.random.uniform(0, 10, 200)
    low_prices = np.minimum(open_prices, close_prices) - np.random.uniform(0, 10, 200)
    
    df = pd.DataFrame({
        'datetime': dates,
        'open': open_prices,
        'high': high_prices,
        'low': low_prices,
        'close': close_prices,
        'volume': np.random.randint(1000, 10000, 200)
    })
    
    fe = FeatureEngineer()
    granular = fe.calculate_granular_minute_features(df)
    
    print(f"\nâœ“ Calculated {len(granular)} granular features")
    
    # Verify key features exist
    expected_features = [
        'avg_liquidity_depth',
        'liquidity_variability', 
        'volume_gini_coefficient',
        'max_momentum_burst',
        'price_reversal_frequency',
        'extreme_move_count_2sigma',
        'volatility_clustering_coef',
        'trading_intensity',
        'price_efficiency_ratio',
        'hourly_vwap_volatility'
    ]
    
    for feature in expected_features:
        if feature in granular:
            print(f"  âœ“ {feature}")
        else:
            print(f"  âœ— {feature} MISSING")
            
    return True


def test_multi_timeframe_enhancements():
    """Test enhanced multi-timeframe analysis"""
    print("\n" + "=" * 60)
    print("Testing Enhanced Multi-Timeframe Analysis")
    print("=" * 60)
    
    # Create sample data with valid OHLC relationships
    dates = pd.date_range(start='2024-01-01 09:30', periods=500, freq='1min')
    open_prices = np.random.uniform(100, 110, 500)
    close_prices = np.random.uniform(100, 110, 500)
    high_prices = np.maximum(open_prices, close_prices) + np.random.uniform(0, 10, 500)
    low_prices = np.minimum(open_prices, close_prices) - np.random.uniform(0, 10, 500)
    
    df = pd.DataFrame({
        'datetime': dates,
        'open': open_prices,
        'high': high_prices,
        'low': low_prices,
        'close': close_prices,
        'volume': np.random.randint(1000, 10000, 500)
    })
    
    fe = FeatureEngineer()
    metrics, frames = fe._multi_timeframe_metrics_and_frames(df)
    
    print(f"\nâœ“ Calculated {len(metrics)} multi-timeframe metrics")
    print(f"âœ“ Generated {len(frames)} timeframe aggregations")
    
    # Verify new timeframes
    expected_timeframes = ['2m', '3m', '5m', '15m', '30m', '1h', '1d']
    print("\nTimeframes:")
    for tf in expected_timeframes:
        if tf in frames:
            print(f"  âœ“ {tf}: {len(frames[tf])} bars")
        else:
            print(f"  âœ— {tf} MISSING")
            
    # Check for cross-timeframe correlations
    if 'timeframe_correlations' in metrics:
        corr = metrics['timeframe_correlations']
        print(f"\nâœ“ Cross-timeframe correlations: {len(corr)} pairs")
        print(f"  Sample: {list(corr.items())[:3]}")
    
    # Check for regime detection per timeframe
    regime_count = sum(1 for k in metrics.keys() if '_regime' in k)
    print(f"\nâœ“ Timeframe-specific regimes: {regime_count}")
    
    return True


def test_full_pipeline_integration():
    """Test that granular features are integrated into full pipeline"""
    print("\n" + "=" * 60)
    print("Testing Full Pipeline Integration")
    print("=" * 60)
    
    # Create sample data with valid OHLC relationships
    dates = pd.date_range(start='2024-01-01 09:30', periods=300, freq='1min')
    open_prices = np.random.uniform(100, 110, 300)
    close_prices = np.random.uniform(100, 110, 300)
    high_prices = np.maximum(open_prices, close_prices) + np.random.uniform(0, 10, 300)
    low_prices = np.minimum(open_prices, close_prices) - np.random.uniform(0, 10, 300)
    
    df = pd.DataFrame({
        'datetime': dates,
        'open': open_prices,
        'high': high_prices,
        'low': low_prices,
        'close': close_prices,
        'volume': np.random.randint(1000, 10000, 300)
    })
    
    fe = FeatureEngineer()
    result = fe.process_full_pipeline(df)
    
    print(f"\nâœ“ Pipeline executed successfully")
    print(f"\nResult keys: {list(result.keys())}")
    
    # Verify granular features are included
    if 'granular_minute_features' in result:
        print(f"\nâœ“ Granular features integrated into pipeline")
        print(f"  Features count: {len(result['granular_minute_features'])}")
    else:
        print(f"\nâœ— Granular features NOT found in pipeline result")
        return False
        
    # Verify enhanced multi-timeframe
    if 'multi_timeframe' in result:
        mtf = result['multi_timeframe']
        print(f"\nâœ“ Multi-timeframe metrics: {len(mtf)}")
        
        # Check for new timeframes
        new_tf_metrics = [k for k in mtf.keys() if any(tf in k for tf in ['2m', '3m', '30m'])]
        if new_tf_metrics:
            print(f"  âœ“ New timeframes present: {len(new_tf_metrics)} metrics")
        
    return True


def main():
    """Run all tests"""
    print("\n" + "ðŸš€ " * 20)
    print("Enhanced Granular Analysis & Visualization - Test Suite")
    print("ðŸš€ " * 20 + "\n")
    
    try:
        # Run tests
        test_granular_features()
        test_multi_timeframe_enhancements()
        test_full_pipeline_integration()
        
        print("\n" + "=" * 60)
        print("âœ… ALL TESTS PASSED!")
        print("=" * 60)
        print("\nEnhancements Summary:")
        print("  âœ“ Granular minute-level analysis features")
        print("  âœ“ Enhanced multi-timeframe aggregations (2m, 3m, 30m)")
        print("  âœ“ Cross-timeframe correlation analysis")
        print("  âœ“ Timeframe-specific regime detection")
        print("  âœ“ Integration into full pipeline")
        print("\nVisualization Panel:")
        print("  âœ“ Created new visualization panel UI")
        print("  âœ“ Interactive charts with PyQt6-Charts")
        print("  âœ“ Profile data exploration capabilities")
        print("  âœ“ Data export functionality")
        print("  âœ“ Integrated into main dashboard")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
